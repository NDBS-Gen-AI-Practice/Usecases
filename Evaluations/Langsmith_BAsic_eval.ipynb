{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import evaluate, Client\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "# 1. Create and/or select your dataset\n",
    "client = Client()\n",
    "dataset = client.clone_public_dataset(\"https://smith.langchain.com/public/728e92ee-b050-4284-93b8-45682ad008f2/d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_concise_enough(root_run: Run, example: Example) -> dict:\n",
    "    root_output = root_run.outputs.get(\"output\", \"\")\n",
    "    example_answer = example.outputs.get(\"output\", \"\")\n",
    "    if root_output and example_answer:\n",
    "        score = len(root_output) < 3 * len(example_answer)\n",
    "    else:\n",
    "        score = False  \n",
    "    return {\"key\": \"is_concise\", \"score\": int(score)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\PROJECTS\\AMS-BOT1\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'my first experiment-2c9077a4' at:\n",
      "https://smith.langchain.com/o/a38aac4e-6076-448b-8973-6807f5f8eeaf/datasets/f54332f3-f098-4a41-ba9b-bfd0f8df473e/compare?selectedSessions=7ab8f2a1-9e49-4adf-bb0a-f10989b08c24\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:01,  4.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.messages</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.output</th>\n",
       "      <th>feedback.is_concise</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>Recommendations for enterprises to accelerate ...</td>\n",
       "      <td>None</td>\n",
       "      <td>There are five recommendations for enterprises...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016797</td>\n",
       "      <td>422c974f-4dc3-47be-8c33-118ab9ee00b5</td>\n",
       "      <td>a6584037-638e-48d5-bb86-cd9840003c4a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>what are the Top Emerging Gen AI Roles in CY 2...</td>\n",
       "      <td>None</td>\n",
       "      <td>Here are the Top Emerging Gen AI Roles in CY 2...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016797</td>\n",
       "      <td>527abb15-f47d-4cb0-98bf-ac40b2aabe42</td>\n",
       "      <td>73f29538-2f43-4123-9432-239b3b852331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>How are companies preparing thnd technology fo...</td>\n",
       "      <td>None</td>\n",
       "      <td>Unfortunately, the provided information does n...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016797</td>\n",
       "      <td>d9c5e7ba-bd3c-4a6a-9896-2f6036d141a8</td>\n",
       "      <td>d2031cf6-c0de-4572-b310-8a351cf43030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>The survey provides data on the budget allocat...</td>\n",
       "      <td>None</td>\n",
       "      <td>Yes, the survey provides data on the budget al...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016797</td>\n",
       "      <td>f3ab39ba-5e76-44df-8243-8ae24db06263</td>\n",
       "      <td>69dce796-ec5e-4e57-aaf1-d56be43380bc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>what are the Top Emerging Gen AI Roles in CY 2...</td>\n",
       "      <td>None</td>\n",
       "      <td>According to the data, the Top Emerging Gen AI...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016797</td>\n",
       "      <td>4eb0805f-c95d-4bcc-9112-b2475b0aeda4</td>\n",
       "      <td>184fe364-3d90-4908-a586-fb75a8f4c4f8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>Recommendations for enterprises to accelerate ...</td>\n",
       "      <td>None</td>\n",
       "      <td>To accelerate digital adoption in the AI and s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016797</td>\n",
       "      <td>ed63e96c-a5c6-455f-ad78-81d737ccd398</td>\n",
       "      <td>5cfbe2eb-a062-41b5-b644-dce0731ceb7a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>what are the Recommendations for enterprises t...</td>\n",
       "      <td>None</td>\n",
       "      <td>According to the Avasant and nasscom Digital E...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016797</td>\n",
       "      <td>60a85602-072a-4a7b-88f8-a3ca2b2a9bc7</td>\n",
       "      <td>d709e558-985d-415b-bce8-5ce0210512c9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults my first experiment-2c9077a4>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_output = lambda x: x[\"messages\"][-1].get(\"content\", \"No query provided\") + \" is a good question. I don't know the answer.\"\n",
    "evaluate(\n",
    "    generate_output,\n",
    "    data=dataset.name,\n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=\"my first experiment\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama(llama3.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def call_ollama(messages, model: str):\n",
    "    response = \"\"\n",
    "    stream = ollama.chat(messages=messages, model=model, stream=True)\n",
    "    for chunk in stream:\n",
    "        #print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "        response += chunk[\"message\"][\"content\"]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_app(inputs: dict) -> dict:\n",
    "    system_msg = \"Answer user questions about this context: \\n\\n\\n\"\n",
    "    if isinstance(inputs[\"messages\"], str):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": inputs[\"messages\"]}\n",
    "        ]\n",
    "    elif isinstance(inputs[\"messages\"], list):\n",
    "        messages = [{\"role\": \"system\", \"content\": system_msg}] + inputs[\"messages\"]\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected format for 'messages'\")\n",
    "    response = call_ollama(messages, model=\"llama3.2\")\n",
    "    \n",
    "    return {\"answer\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': \"The main differences in training efficiency between a graphics card (GPU) and Random Access Memory (RAM) depend on the type of task, specifically deep learning model training. Here's a summary:\\n\\n**Graphics Card (GPU):**\\n\\n1. **Parallel processing**: GPUs are designed for parallel processing, making them well-suited for matrix multiplications, convolutional operations, and other compute-intensive tasks commonly used in deep learning.\\n2. **Massive parallelism**: With hundreds or thousands of cores, GPUs can process vast amounts of data simultaneously, reducing the training time for large models.\\n3. **High throughput**: GPUs can handle high-throughput data transfer rates, which is essential for efficient model training.\\n\\nHowever, there are some limitations:\\n\\n1. **Memory bandwidth**: GPUs have limited memory bandwidth, which can bottleneck memory-intensive tasks like batch normalization and data loading.\\n2. **Cache hierarchy**: GPUs often rely on slower cache hierarchies compared to CPUs, leading to increased latency and reduced performance.\\n\\n**Random Access Memory (RAM):**\\n\\n1. **Direct access**: RAM provides direct access to data, reducing the overhead of caching and memory management.\\n2. **Higher storage density**: RAM has a higher storage density than GPUs, making it more suitable for tasks that require sequential access to large datasets.\\n3. **Less parallelism**: RAM is designed for sequential processing, not parallel processing.\\n\\nHowever, there are some limitations:\\n\\n1. **Lower throughput**: RAM generally offers lower throughput compared to GPUs, especially when working with large datasets.\\n2. **Higher latency**: RAM has higher latency compared to GPUs due to the slower access times and fewer cores.\\n\\n**In summary:**\\n\\n* Use a GPU for:\\n\\t+ Large-scale deep learning model training\\n\\t+ Compute-intensive tasks like matrix multiplications and convolutional operations\\n\\t+ High-throughput data transfer rates\\n* Use RAM for:\\n\\t+ Sequential processing tasks like data loading and batch normalization\\n\\t+ Applications with high storage density requirements (e.g., video editing, scientific simulations)\\n\\t+ When direct access to data is critical\\n\\nKeep in mind that these are general guidelines, and the best choice between GPU and RAM depends on your specific use case, dataset size, and model complexity.\"}\n"
     ]
    }
   ],
   "source": [
    "# Test the function with the correct input key\n",
    "result = my_app(\n",
    "    {\n",
    "        \"messages\": \"What are the main differences in training efficiency between graphic card and ram\"\n",
    "    }\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'llama3.2-Evaluations-d8c0816f' at:\n",
      "https://smith.langchain.com/o/a38aac4e-6076-448b-8973-6807f5f8eeaf/datasets/f54332f3-f098-4a41-ba9b-bfd0f8df473e/compare?selectedSessions=96df9d1f-96be-41a4-8410-cbfbce87449d\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:51, 55.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ExperimentResults llama3.2-Evaluations-d8c0816f>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "def is_answered(run: Run, example: Example) -> dict:\n",
    "    student_answer = run.outputs.get(\"answer\")\n",
    "    if not student_answer:\n",
    "        return {\"key\": \"is_answered\", \"score\": 0}\n",
    "    else:\n",
    "        return {\"key\": \"is_answered\", \"score\": 1}\n",
    "qa_evalulator = [is_answered]\n",
    "experiment_results = evaluate(\n",
    "    my_app,\n",
    "    data=dataset.name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"llama3.2-Evaluations\",\n",
    ")\n",
    "print(experiment_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation using groq(llama3-8b-1092)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "# Initialize Groq client with API key\n",
    "groq_client = Groq(api_key=\"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\")\n",
    "\n",
    "# Few-shot prompt template\n",
    "few_shot_prompt = \"\"\"\n",
    "You are an AI assistant that answers questions about digital transformation, industry trends, and strategic technology roles. Answer the following questions concisely in 2-3 sentences based on the provided context. Provide direct answers based on the examples without adding unnecessary prefaces.\n",
    "\n",
    "### Example 1\n",
    "**Question**: How are companies adapting their digital strategies to sustain growth amid fluctuating tech budgets?\n",
    "**Answer**: Organizations are prioritizing automation and data insights, reallocating resources to maintain growth despite budget cuts.\n",
    "\n",
    "### Example 2\n",
    "**Question**: Which industries have shown significant advancement in adopting automation technologies recently?\n",
    "**Answer**: Manufacturing and energy sectors lead in automation, boosting efficiency and cutting costs.\n",
    "\n",
    "### Example 3\n",
    "**Question**: How is the growing role of AI specialists transforming business decision-making?\n",
    "**Answer**: AI specialists enable real-time, data-driven decisions, helping companies stay responsive to market needs.\n",
    "\n",
    "### Example 4\n",
    "**Question**: What role does vendor consolidation play in modernizing digital infrastructure?\n",
    "**Answer**: Vendor consolidation simplifies tech management, allowing companies to adopt innovations faster and save costs.\n",
    "\n",
    "### Example 5\n",
    "**Question**: How is Generative AI influencing personalized customer experiences?\n",
    "**Answer**: Generative AI personalizes experiences by analyzing customer data, widely adopted in retail to increase engagement.\n",
    "\n",
    "### Example 6\n",
    "**Question**: In what ways are digital transformation initiatives impacting the financial services industry?\n",
    "**Answer**: Financial services focus on digital channels and automation, leading to improved customer satisfaction and regulatory response.\n",
    "\n",
    "### Example 7\n",
    "**Question**: Why are Chief Technology Officers focusing more on digital maturity in organizations?\n",
    "**Answer**: CTOs emphasize digital maturity to use data, AI, and cloud effectively, aligning digital strategy with business goals.\n",
    "\n",
    "### Example 8\n",
    "**Question**: How are educational institutions supporting the need for digital talent in tech-focused sectors?\n",
    "**Answer**: Universities partner with tech firms to develop programs that address skill gaps in AI, data science, and cybersecurity.\n",
    "\n",
    "### Example 9\n",
    "**Question**: What is the importance of AI ethics in the deployment of enterprise AI solutions?\n",
    "**Answer**: AI ethics ensures fair, transparent solutions aligned with social values, especially critical in healthcare and finance.\n",
    "\n",
    "### Example 10\n",
    "**Question**: How is digital spending expected to evolve in response to sustainability goals?\n",
    "**Answer**: Organizations are investing in sustainable tech like IoT and AI for energy management, reducing costs and supporting environmental goals.\n",
    "\n",
    "### Question\n",
    "**Question**: {question}\n",
    "**Answer**:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "# Initialize Groq client with API key\n",
    "groq_client = Groq(api_key=\"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\")\n",
    "\n",
    "def my_app(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Generates answers to user questions based on the provided context using the Groq API.\n",
    "    \"\"\"\n",
    "    system_msg = \"You are a helpful assistant for document-based queries. \\n\\n\\n\"\n",
    "    if isinstance(inputs[\"messages\"], str):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": inputs[\"messages\"]}\n",
    "        ]\n",
    "    elif isinstance(inputs[\"messages\"], list):\n",
    "        messages = [{\"role\": \"system\", \"content\": system_msg}] + inputs[\"messages\"]\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected format for 'messages'\")\n",
    "    try:\n",
    "        response = groq_client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=\"llama3-8b-8192\",\n",
    "            max_tokens=1024,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return {\"answer\": response.choices[0].message.content}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"An error occurred while generating the response: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'According to a recent report by the Digital McKinsey Global Survey, the sectors that showed the most improvement in digital maturity in 2023 are:\\n\\n1. Healthcare: The healthcare sector has made significant strides in digital maturity, with 64% of respondents reporting improvement in digital capabilities, particularly in areas such as patient engagement, data analytics, and telemedicine.\\n2. Financial Services: The financial services sector has also seen significant improvement in digital maturity, with 58% of respondents reporting improvement in areas such as customer onboarding, payment processing, and digital lending.\\n3. Retail and Consumer Goods: The retail and consumer goods sector has made notable progress in digital maturity, with 55% of respondents reporting improvement in areas such as e-commerce, supply chain management, and customer experience.\\n4. Manufacturing: The manufacturing sector has also shown improvement in digital maturity, with 52% of respondents reporting improvement in areas such as production planning, quality control, and supply chain management.\\n5. Media and Entertainment: The media and entertainment sector has seen a significant jump in digital maturity, with 50% of respondents reporting improvement in areas such as content distribution, personalization, and audience engagement.\\n\\nThese sectors have made significant investments in digital technologies, including cloud computing, artificial intelligence, and data analytics, which has enabled them to improve their digital capabilities and stay competitive in the market.\\n\\nHere are some key statistics from the report:\\n\\n* 75% of respondents reported that their organizations have made significant investments in digital technologies in the past two years.\\n* 60% of respondents reported that their organizations have seen significant improvements in digital capabilities, such as customer engagement, operational efficiency, and revenue growth.\\n* 55% of respondents reported that their organizations have implemented artificial intelligence and machine learning technologies to improve decision-making and automate processes.\\n\\nOverall, the report suggests that many sectors have made significant progress in digital maturity, but there is still room for improvement, particularly in areas such as cybersecurity, data governance, and digital culture.'}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "answer = my_app({\"messages\": \"What sectors showed the most improvement in digital maturity in 2023?\"})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Groq-evaluations-088f70b2' at:\n",
      "https://smith.langchain.com/o/a38aac4e-6076-448b-8973-6807f5f8eeaf/datasets/f54332f3-f098-4a41-ba9b-bfd0f8df473e/compare?selectedSessions=db729d62-efce-49be-8a6b-35ff097429c7\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:01,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: <ExperimentResults Groq-evaluations-088f70b2>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# Correctness metric\n",
    "def correctness_metric(run: Run, example: Example) -> dict:\n",
    "    student_answer = run.outputs.get(\"output\")\n",
    "    correct_answer = example.outputs.get(\"output\")\n",
    "    if student_answer == correct_answer:\n",
    "        return {\"key\": \"correctness\", \"score\": 1}\n",
    "    else:\n",
    "        return {\"key\": \"correctness\", \"score\": 0}  \n",
    "\n",
    "\n",
    "# Define your evaluation function\n",
    "qa_evaluators = [correctness_metric]\n",
    "experiment_results = evaluate(\n",
    "    my_app, \n",
    "    data=dataset.name,  \n",
    "    evaluators=qa_evaluators,\n",
    "    experiment_prefix=\"Groq-evaluations\",\n",
    "    metadata={\"variant\": \"stuff website context\"}\n",
    ")\n",
    "print(\"Evaluation Results:\", experiment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Groq-evaluations-d2863b81' at:\n",
      "https://smith.langchain.com/o/a38aac4e-6076-448b-8973-6807f5f8eeaf/datasets/f54332f3-f098-4a41-ba9b-bfd0f8df473e/compare?selectedSessions=f82385cf-b518-47a5-ad57-c1174427fd8f\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:01,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: <ExperimentResults Groq-evaluations-d2863b81>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# Faithfulness metric\n",
    "def faithfulness(root_run: Run, example: Example) -> dict:\n",
    "    root_context = root_run.inputs.get(\"context\", \"\").strip().lower()\n",
    "    root_output = root_run.outputs.get(\"output\", \"\").strip().lower()\n",
    "    \n",
    "    faithfulness_score = int(all(word in root_context for word in root_output.split()))\n",
    "    return {\"key\": \"faithfulness\", \"score\": faithfulness_score}\n",
    "\n",
    "def is_answered(run: Run, example: Example) -> dict:\n",
    "    student_answer = run.outputs.get(\"answer\")\n",
    "    if not student_answer:\n",
    "        return {\"key\": \"is_answered\", \"score\": 0}\n",
    "    else:\n",
    "        return {\"key\": \"is_answered\", \"score\": 1}\n",
    "\n",
    "# List of all evaluators\n",
    "qa_evaluators = [\n",
    "    faithfulness,is_answered\n",
    "]\n",
    "\n",
    "# Run the evaluation\n",
    "experiment_results = evaluate(\n",
    "    my_app,\n",
    "    data=dataset.name,\n",
    "    evaluators=qa_evaluators,\n",
    "    experiment_prefix=\"Groq-evaluations\",\n",
    ")\n",
    "\n",
    "print(\"Evaluation Results:\", experiment_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
